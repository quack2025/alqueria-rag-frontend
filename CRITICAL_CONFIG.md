# ‚ö†Ô∏è CONFIGURACI√ìN CR√çTICA - ALQUER√çA RAG SYSTEM

## üî¥ PROBLEMAS CR√çTICOS Y SOLUCIONES

### 1. **L√≠mites de Tokens y Timeouts**

#### Problema
- Claude trunca respuestas cuando exceden l√≠mite de tokens
- Vercel Functions timeout despu√©s de 5 minutos
- Evaluaciones largas (5+ personas) pueden fallar

#### Configuraci√≥n Actual
```javascript
// consolidatedEvaluationService.ts
max_tokens: 32000  // M√°ximo soportado
temperature: 0.2   // Baja para JSON consistente

// vercel.json
maxDuration: 300   // 5 minutos m√°ximo

// API timeouts
RAG queries: 60 segundos
Claude calls: Sin timeout expl√≠cito (PROBLEMA)
```

#### ‚úÖ Soluciones Implementadas
1. **Sistema de Fallback 3 niveles**
   - Nivel 1: Prompt completo (32K tokens)
   - Nivel 2: Prompt compacto (4K tokens)
   - Nivel 3: An√°lisis local de emergencia

2. **Detecci√≥n de truncamiento**
   ```javascript
   if (data.stop_reason === 'max_tokens') {
     // Activar fallback autom√°tico
   }
   ```

3. **Limpieza de respuestas**
   - Funci√≥n `cleanClaudeResponse()` elimina markdown
   - Validaci√≥n JSON antes de parsear

### 2. **Seguridad de API Keys**

#### ‚ö†Ô∏è CR√çTICO
- NUNCA poner `VITE_CLAUDE_API_KEY` en el frontend
- SIEMPRE usar Vercel Functions como proxy

#### Configuraci√≥n Segura
```bash
# .env.local (NUNCA commitear)
VITE_USE_VERCEL_FUNCTIONS=true  # Forzar uso de proxy

# En Vercel Dashboard
CLAUDE_API_KEY=sk-ant-xxx  # Solo en servidor
```

### 3. **Rate Limiting**

#### Problema
- Claude API: 60 requests/min
- Sin retry logic = fallos aleatorios

#### Soluci√≥n
```javascript
import { callClaudeWithRetry } from './utils/retryWithBackoff';

// Usar wrapper con retry
const response = await callClaudeWithRetry(
  () => fetch('/api/claude-evaluation', {...}),
  'consolidaci√≥n de reporte'
);
```

### 4. **Performance**

#### Problemas Detectados
- Bundle size: 784KB (muy grande)
- Evaluaciones secuenciales (muy lento)
- Sin cach√© de respuestas

#### Optimizaciones Pendientes
```javascript
// Paralelizaci√≥n controlada
const batchSize = 2; // Max 2 personas en paralelo
for (let i = 0; i < personas.length; i += batchSize) {
  const batch = personas.slice(i, i + batchSize);
  await Promise.all(batch.map(p => interview(p)));
}

// Implementar cach√©
const cacheKey = `${concept.id}_${persona.id}`;
if (cache.has(cacheKey)) return cache.get(cacheKey);
```

## üö® MONITOREO Y DEBUGGING

### Logs Cr√≠ticos a Revisar

```javascript
// Buscar estos patrones en console:
"‚ùå Estructura de respuesta inesperada"  // JSON malformado
"‚ö†Ô∏è Respuesta truncada"                  // L√≠mite de tokens
"‚ùå FALLBACK REPORT ACTIVADO"            // Sistema de respaldo
"Rate limit alcanzado"                   // Throttling de API
```

### M√©tricas a Monitorear

1. **Tasa de √âxito**
   - Target: >95% evaluaciones completas
   - Actual: ~80% (mejorar)

2. **Tiempos de Respuesta**
   - Entrevista individual: 30-45 seg
   - Consolidaci√≥n: 20-30 seg
   - Total 5 personas: <5 min

3. **Uso de Tokens**
   - Por entrevista: ~2000-3000
   - Consolidaci√≥n: ~8000-15000
   - Costo estimado: $0.10-0.20 por evaluaci√≥n completa

## üìã CHECKLIST PRE-PRODUCCI√ìN

### Antes de Desplegar

- [ ] Verificar `VITE_USE_VERCEL_FUNCTIONS=true`
- [ ] API keys solo en Vercel Dashboard
- [ ] Retry logic implementado en todos los servicios
- [ ] Timeouts configurados correctamente
- [ ] Fallbacks probados manualmente
- [ ] Bundle size optimizado (<500KB)
- [ ] Logs de error configurados

### Testing Cr√≠tico

```bash
# 1. Test de carga
- Evaluar 5 personas simult√°neamente
- Verificar que no hay timeouts
- Confirmar fallbacks funcionan

# 2. Test de resiliencia
- Simular fallo de Claude API
- Verificar activaci√≥n de fallback local
- Confirmar UX no se rompe

# 3. Test de seguridad
- Inspeccionar bundle.js en producci√≥n
- Verificar NO hay API keys expuestas
- Confirmar todas las llamadas van por proxy
```

## üî• TROUBLESHOOTING

### Error: "Estructura de respuesta inesperada"
```javascript
// Causa: Claude retorn√≥ markdown o JSON truncado
// Soluci√≥n:
1. Verificar stop_reason en respuesta
2. Si es 'max_tokens', reducir prompt
3. Activar fallback compacto
```

### Error: "429 Too Many Requests"
```javascript
// Causa: Rate limit de Claude
// Soluci√≥n:
1. Implementar retry con backoff
2. Reducir paralelizaci√≥n
3. A√±adir delays entre llamadas
```

### Error: Timeout en Vercel
```javascript
// Causa: Evaluaci√≥n toma >5 min
// Soluci√≥n:
1. Reducir n√∫mero de personas
2. Optimizar prompts
3. Considerar arquitectura de queue
```

## üí° MEJORAS FUTURAS

1. **Queue System**
   - Bull/Redis para procesar evaluaciones
   - Webhooks para notificar completado
   - Sin l√≠mite de timeout

2. **Streaming Responses**
   - Server-Sent Events para progreso real-time
   - Respuestas parciales visibles

3. **Edge Functions**
   - Migrar a Vercel Edge (no timeout)
   - Menor latencia global

4. **Cach√© Inteligente**
   - Redis para respuestas frecuentes
   - Invalidaci√≥n por concepto/persona

---

**√öltima actualizaci√≥n:** 17 Enero 2025
**Mantenedor:** Jorge con asistencia Claude
**Criticidad:** ALTA - Revisar antes de cada deploy